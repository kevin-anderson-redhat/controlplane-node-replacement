= ControlPlane Node Replacement in Agent Installed Cluster without DHCP and BMC credentials
:toc:

== Introduction
The following procedure describes replacing an OpenShift Container Platform (OCP) ControlPlane node that has failed and is powered off.
This procedure assumes that the ControlPlane was originally deployed
using the Agent Installation procedure without providing BMC credentials in the agent config yaml.
For clarity, the installation the OCP cluster required that each node booted the generated Agent Installer ISO thru BMC (iLO or iDRAC) attached virtual media,
network boot, or temporarily attached drives (e.g., USB thumb drive).

== Prerequisites

* You have identified the unhealthy bare metal etcd member.
* You have verified that either the machine is not running or the node is not ready.
* You have access to the cluster as a user with the cluster-admin role.
* You have taken an etcd backup.
* Downloads:
** coreos-installer
** Red Hat CoreOS live iso

[Important]
====
You must take an etcd backup before performing this procedure so that your cluster can be restored if you encounter any issues.
====

For this type of installation there will be no machineset for the control plane.
Run the following command to look for machine sets.

```
oc get machinesets -n openshift-machine-api
```

There should only be one or more machinesets for the workers. 
If a machine set exists for the control plane, do not use the following procedure.

== Procedure

=== Remove the unhealthy member <<ocp>>

. Choose an etcd pod that is not on the affected node using the following command:
+
```
oc -n openshift-etcd get pods -l k8s-app=etcd -o wide
```

. Connect to the running etcd container, passing in the name of a pod that is not on the failed node:
+
```
oc rsh -n openshift-etcd etcd-old-controlplane-1
```

. View the member list:
+
```
etcdctl member list -w table
```
+
.. Example output
+
```
+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| 6fc1e7c9db35841d | started | ip-10-0-131-183.ec2.internal | https://10.0.131.183:2380 | https://10.0.131.183:2379 |
| 757b6793e2408b6c | started | ip-10-0-164-97.ec2.internal  | https://10.0.164.97:2380  | https://10.0.164.97:2379  |
| ca8c2990a0aa29d1 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+
```

.. Take note of the ID and the name of the unhealthy etcd member, because these values are required later in the procedure. The etcdctl endpoint health command will list the removed member until the replacement procedure is completed and the new member is added.

. Remove the unhealthy etcd member by providing the ID note above to the etcdctl member remove command:
+
```
etcdctl member remove 7a8197040a5126c8
```
+
.. Example output
+
```
Member 7a8197040a5126c8 removed from cluster b23536c33f2cdd1b
```

. View the member list again and verify that the member was removed:
+
```
etcdctl member list -w table
```
.. Example output
+
```
+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| 757b6793e2408b6c | started |  ip-10-0-164-97.ec2.internal |  https://10.0.164.97:2380 |  https://10.0.164.97:2379 |
| ca8c2990a0aa29d1 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+
```

IMPORTANT: After you remove the member, the cluster might be unreachable for a short time while the remaining etcd instances reboot.

=== Turn off etcd quorum guard <<ocp>>

. Run the following command to turn of the etcd quorum guard:
+
```
oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'
```

NOTE: The refrenced OpenShift documentation <<ocp>> includes a step to delete the node.  However, as noted later in the documentation, 
removing the bmh and machine will cause the operator to remove the node.

//
// === Delete the affected node <<ocp>>
//
// . Run the following command to delete the affected node
// +
// ```
// oc delete node <node_name> <1>
// ```
// <1> replace `<node_name>` with the name of the failed/off node

=== Delete affected node secrets <<ocp>>

. List the secrets for the unhealthy etcd member that was removed.
+
```
oc get secrets -n openshift-etcd | grep <node_name> <1>
```
<1> replace `<node_name>` with the name of the failed/off node (deleted in previous step)

. There should be three secrets associated with the affected node whose names start with the following
** etcd-peer-
** etcd-serving-metrics-
** etcd-serving-

. Delete the secrets associated with the affected node that was removed.

.. Delete the peer secret:
+
```
oc delete secret -n openshift-etcd etcd-peer-<node_name>  <1>
```
<1> replace `<node_name>` with the name of the affected node


.. Delete the serving secret:
+
```
oc delete secret -n openshift-etcd etcd-serving-<node_name>  <1>
```
<1> replace `<node_name>` with the name of the affected node


.. Delete the metrics secret:
```
oc delete secret -n openshift-etcd etcd-serving-metrics-<node_name>  <1>
<1> replace `<node_name>` with the name of the affected node
```

NOTE: For this type of installation there should be no ControlPlane machine set

=== Obtain the machine for the affected node

. Runn the following ommand to identify the machine associated with the affected node
+
```
oc get machines -n openshift-machine-api -o wide
```
** Example Output
+
```
NAME                              PHASE     TYPE   REGION   ZONE   AGE     NODE                               PROVIDERID                                                                                              STATE
examplecluster-control-plane-0    Running                          3h11m   openshift-control-plane-0   baremetalhost:///openshift-machine-api/openshift-control-plane-0/da1ebe11-3ff2-41c5-b099-0aa41222964e   externally provisioned 
examplecluster-control-plane-1    Running                          3h11m   openshift-control-plane-1   baremetalhost:///openshift-machine-api/openshift-control-plane-1/d9f9acbc-329c-475e-8d81-03b20280a3e1   externally provisioned
examplecluster-control-plane-2    Running                          3h11m   openshift-control-plane-2   baremetalhost:///openshift-machine-api/openshift-control-plane-2/3354bdac-61d8-410f-be5b-6a395b056135   externally provisioned
examplecluster-compute-0          Running                          165m    openshift-compute-0         baremetalhost:///openshift-machine-api/openshift-compute-0/3d685b81-7410-4bb3-80ec-13a31858241f         provisioned
examplecluster-compute-1          Running                          165m    openshift-compute-1         baremetalhost:///openshift-machine-api/openshift-compute-1/0fdae6eb-2066-4241-91dc-e7ea72ab13b9         provisioned
```

. Ensure that the Bare Metal Operator is available by running the following command:
+
```
oc get clusteroperator baremetal
```
+
Example output
+
```
NAME        VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
baremetal   4.18.0    True        False         False      3d15h
```

. Save the affected BareMetalHost object to a file for later use by running the following command
+
```
oc get -n openshift-machine-api bmh <node_name>  >bmh_affected.yaml <1>
```
<1> replace `<node_name>` with the name of the affected node (which normally matches the associate bmh name).

. Remove the affected BareMetalHost object by running the following command
+
```
oc delete -n openshift-machine-api bmh <node_name>  <1>
```
<1> replace `<node_name>` with the name of the affected node (which normally matches the associate bmh name).

. Delete the machine of the unhealthy member by running the following command:
+
```
oc delete machine -n openshift-machine-api <machine_name> <1>
```
<1> replace <machine_name> with the machine name found previously to be associated with the affected node.

NOTE: After you remove the BareMetalHost and Machine objects, the Machine controller automatically deletes the Node object.

.. If deletion of the machine is delayed for any reason or the command is obstructed and delayed, you can force deletion by removing the machine object finalizer field.

[Important]
====
Do not interrupt machine deletion by pressing Ctrl+c. 
You must allow the command to proceed to completion.
Open a new terminal window to edit and delete the finalizer fields.
====

. Wait for clusteroperatators to complete their updates
+
```
watch oc get clusteroperators
```

=== Create a new BareMetalHost

. Edit the saved bmh_affected.yaml file from the earlier step
.. Remove the following metadata items
... creationTimestamp
... generation
... resourceVersion
... uid
.. Remove the status section
.. The resulting file should resemble the following
+
```
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: openshift-control-plane-2
  namespace: openshift-machine-api
spec:
  automatedCleaningMode: disabled
  bmc:
    address:
    credentialsName:
    disableCertificateVerification: true
  bootMACAddress: 48:df:37:b0:8a:a0
  bootMode: UEFI
  externallyProvisioned: true
  online: true
  rootDeviceHints:
    deviceName: /dev/disk/by-id/scsi-<serial_number>
  userData:
    name: master-user-data-managed
    namespace: openshift-machine-api
```
.. The following warning is expected upon creation of the BareMetalHost
+
```
Warning: metadata.finalizers: "baremetalhost.metal3.io": prefer a domain-qualified finalizer name to avoid accidental conflicts with other finalizer writers
```

=== Create New ControlPlane Node


. Extract the controlplane ignition secret using the following commands that include removal of the starting userData line.
+
```
oc extract secret/master-user-data-managed \
           -n openshift-machine-api \
           --keys=userData \
           --to=- \
| sed '/^userData/d' > new_controlplane.ign
```

. Create an nmstate file similar to the sample below for the new node's network configuration.
+
```
interfaces:
  - name: eno1
    type: ethernet
    state: up
    mac-address: "ab:cd:ef:01:02:03"
    ipv4:
      enabled: true
      address:
        - ip: 192.168.20.11
          prefix-length: 24
      dhcp: false
    ipv6:
      enabled: false
dns-resolver:
  config:
    search:
      - iso.sterling.home
    server:
      - 192.168.20.8
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.20.1
    next-hop-interface: eno1
    table-id: 254

```
+
NOTE: The failed node's networkConfig section in the agent-config.yaml from the original cluster deployment can be used as a starting point for the new controlplane node's nmstate file.
The following illustrates extraction of that section for the first control plane node.
+
```
cat agent-config-iso.yaml | yq .hosts[0].networkConfig > new_controlplane_nmstate.yaml
```

. Create the customized Red Hat CoreOS live ISO with the following commands+
+
```
coreos-installer iso customize rhcos-live.86_64.iso \
--dest-ignition new_controlplane.ign \
--network-nmstate new_controlplane_nmstate.yaml \
--dest-device /dev/disk/by-path/xxxxx \
-f
```

. Boot the new ControlPlane node with the customized Red Hat CoreOS live ISO.

. Approve the Certificate Signing Requests (CSR) to join the new node to the cluster.

=== Link Node, BareMetalHost and Machine

NOTE: This procedure is adapted from https://access.redhat.com/solutions/6471021[Solution 6471021]

. Generate the providerID lines for control plane nodes
+
```
oc get -n openshift-machine-api baremetalhost \
        -ojson | jq -r '.items[] | '\
'"providerID: baremetalhost:///openshift-machine-api/"'\
' + .metadata.name + "/" + .metadata.uid'
```

. Identify the cluster
+
```
oc get machine -n openshift-machine-api \
   -l machine.openshift.io/cluster-api-machine-role=master \
   -L machine.openshift.io/cluster-api-cluster
```

. Create a Machine for the new ContrlPlane Node
(adjust the machine object name as required)
+
```
cat <<EOF | oc apply -f -
apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  annotations:
    metal3.io/BareMetalHost: openshift-machine-api/new-cp-2  <1>
  finalizers:
    - machine.machine.openshift.io
  labels:
    machine.openshift.io/cluster-api-cluster: bmtest-extpr <2>
    machine.openshift.io/cluster-api-machine-role: master
    machine.openshift.io/cluster-api-machine-type: master
  name: new-controlplane-machine <3>
  namespace: openshift-machine-api
spec:
  metadata: {}
  providerID:  <4>
  providerSpec:
    value:
      apiVersion: baremetal.cluster.k8s.io/v1alpha1
      hostSelector: {}
      image:
        checksum: ""
        url: ""
      kind: BareMetalMachineProviderSpec
      userData:
        name: master-user-data-managed
EOF
```
<1> replace new-cp-2 with the name of the new BareMetalHost created for the new node
<2> replace bmtest-extpr with the `CLUSTER-API-CLUSTER` value shown in the previous step for the other control plane machines
<3> replace `new-controlplane-machine` with the name of the machine to be created.  This be similar to the names shown in the previous step.
<4> put the name of the new baremetal host here
<5> put the uid of the new baremetal host here
+
.. The following warning is expected:
+
```
Warning: metadata.finalizers: "machine.machine.openshift.io": prefer a domain-qualified finalizer name to avoid accidental conflicts with other finalizer writers
```

. Link the new ControlPlane Node and Machine to the BareMetalHost
(adjust the machine object name as required)
+
```
export NEW_NODE_NAME=new-controlplane
export NEW_MACHINE_NAME=new-machine

export BMH_UID=$(oc get bmh $NEW_NODE_NAME -ojson | jq -r .metadata.uid)

oc patch node NEW_NODE_NAME \ <1>
--type merge \
--patch \
'{"spec":{"providerID":"baremetalhost://openshift-machine-api/'\
$NEW_NODE_NAME\
'/'\
$BMH_UID\
'"}}'

oc patch bmh NEW_NODE_NAME --type merge \ <1>
  --patch \
'{'\
'  "spec": {'\
'    "consumerRef": {'\
'      "apiVersion":"machine.openshift.io/v1beta1",'\
'      "kind":"Machine",'\
'      "name":"'NEW_NODE_NAME'",'\ <1>
'      "namespace": "openshift-machine-api"'\
'    }'\
  '}'\
'}'
```
<1> Replace `NEW_NODE_NAME`

.. Example BareMetalHost spec
+
```
 spec:
    automatedCleaningMode: metadata
    bmc:
      address: ""
      credentialsName: ""
    bootMACAddress: ab:cd:ef:01:02:03
    bootMode: UEFI
    consumerRef:
      apiVersion: machine.openshift.io/v1beta1
      kind: Machine
      name: new-controlplane
      namespace: openshift-machine-api
    externallyProvisioned: true
    hardwareProfile: unknown
    online: true
    userData:
      name: master-user-data-managed
      namespace: openshift-machine-api
```
. Set bmh poweredOn
+
```
oc patch bmh $NEW_NODE_NAME \
   --subresource status \
   --type json \
   -p \
'['\
'  {'\
'    "op": "replace",'\
'    "path": "/status/poweredOn",'\
'    "value": true'\
'  }'\
']'
```

. Set bmh unmanaged?
+
```
oc patch bmh $NEW_NODE_NAME \
   --subresource status \
   --type json \
   -p \
'['\
'  {'\
'    "op": "replace",'\
'    "path": "/status/provisioning/state",'\
'    "value": "unmanaged"'\
'  }'\
']'
```

. Set the provisioned state
(adjust the machine object name as required)
+
```
oc  patch machines new-controlplane-machine \
    -n openshift-machine-api \
    --subresource status \
    --type json \
    -p \
'['\
'  {'\
'    "op": "replace",'\
'    "path": "/status/phase",'\
'    "value": "Provisioned"'\
'  }'\
']'
```

=== Add The New Etcd Member

. Choose an etcd pod from those returned by the following command:
+
```
oc -n openshift-etcd get pods -l k8s-app=etcd -o wide
```

. Connect to the running etcd container, passing in the name of a pod that is not on the failed node:
+
```
oc rsh -n openshift-etcd etcd-old-controlplane-1
```

. View the member list:
+
```
etcdctl member list -w table
```

. Add the new contrlplane etcd member
+
```
oc rsh -n openshift-etcd etcd-oldcontrolplane
etcdctl member list -w table
etcdctl member add new-controlplane --peer-urls="https://192.168.20.42:2380"
exit
```

. Force etcd redeployment
+
```
oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'
```
.. This is taken from step 5 of https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/backup_and_restore/control-plane-backup-and-restore#restore-replace-crashlooping-etcd-member_replacing-unhealthy-etcd-member[6.2.4.2. Replacing an unhealthy etcd member whose etcd pod is crashlooping]

. Turn the quorum guard back on
+
```
oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'
```


[bibliography]
== References
* [[[ocp]]] Red Hat OpenShift Container Platform Documentation - Configure - Backup and Restore - 
https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/backup_and_restore/control-plane-backup-and-restore#restore-replace-stopped-baremetal-etcd-member_replacing-unhealthy-etcd-member
* [[[bmh]]] *BareMetalHost reference is missing after adding a host to Openshift Assisted Installer cluster* https://access.redhat.com/solutions/6471021
* [[[rig]]] *How to retrieve Master or Worker Ignition Configuration from OpenShift Container Platform 4?* https://access.redhat.com/solutions/5504291
